{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2409882",
   "metadata": {},
   "source": [
    "# Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de34edda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhilmarepally/.pyenv/versions/3.10.12/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embeddings: tensor([[ 0.0559,  0.0093,  0.0912,  ..., -0.0607,  0.2878, -0.1385],\n",
      "        [ 0.0922, -0.2819,  0.3255,  ..., -0.1049,  0.2217, -0.3235]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "'''     \n",
    "    For task1, we have to implement a sentence transformer, I chose pre-trained BERT model (works well for most the NLP tasks)\n",
    "\tThe model processes input text, extracts token-level embeddings from BERT, and applies an adaptive average pooling layer to obtain fixed-size        sentence representations. The tokenizer converts input sentences into tensors with padding and truncation for uniform input processing. \n",
    "    Finally, the model generates sentence embeddings, which can be used for NLP tasks like similarity detection and classification.\n",
    "\n",
    "'''\n",
    "class SentenceTransformer(torch.nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.pooling = torch.nn.AdaptiveAvgPool1d(1) #torch.nn.AdaptiveAvgPool1d(1) is used to convert variable-length token embeddings into a fixed-size sentence embedding.\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  \n",
    "        embeddings = self.pooling(last_hidden_state.permute(0, 2, 1)).squeeze(2)  \n",
    "        return embeddings\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = SentenceTransformer()\n",
    "\n",
    "sentences = [\"AI is transforming every Industry\", \"AI Agents are going to be the next big thing\"]\n",
    "inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "embeddings = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "print(\"Sentence Embeddings:\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1baea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embeddings: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence Embeddings:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366fa278",
   "metadata": {},
   "source": [
    "# Model Choice: I chose BERT as it does a pretty good job in various NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1bb7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Logits: tensor([[ 0.2969,  0.2258, -0.3647],\n",
      "        [ 0.1419,  0.1071, -0.3310]], grad_fn=<AddmmBackward0>)\n",
      "NER Logits: tensor([[[-0.4228,  0.4767, -0.3502],\n",
      "         [-0.0603,  0.5656,  0.1865],\n",
      "         [-0.2978, -0.0703, -0.2870],\n",
      "         [ 0.0303,  0.1853, -0.2249],\n",
      "         [ 0.1140,  0.2757, -0.4664],\n",
      "         [ 0.0982, -0.0459,  0.0544],\n",
      "         [ 0.2256,  0.2150,  0.0583],\n",
      "         [ 0.1169,  0.0808, -0.0017],\n",
      "         [ 0.0717,  0.0349,  0.0260],\n",
      "         [ 0.1206,  0.1475,  0.0597],\n",
      "         [ 0.0673, -0.0122, -0.0017],\n",
      "         [ 0.1185, -0.0009,  0.0132]],\n",
      "\n",
      "        [[-0.3958,  0.3844, -0.2965],\n",
      "         [-0.1042,  0.5323,  0.1365],\n",
      "         [ 0.1411,  0.0420, -0.3621],\n",
      "         [ 0.0907, -0.2787,  0.0215],\n",
      "         [ 0.2288, -0.3820,  0.0359],\n",
      "         [-0.1557, -0.3091,  0.1784],\n",
      "         [ 0.0955, -0.1547,  0.2409],\n",
      "         [ 0.2524, -0.3988, -0.0172],\n",
      "         [ 0.3267, -0.0236,  0.0009],\n",
      "         [ 0.2136, -0.1029, -0.0035],\n",
      "         [ 0.0673, -0.1791,  0.0398],\n",
      "         [ 0.0999,  0.2665,  0.0722]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskLearningTransformer(torch.nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', num_sentence_classes=3, num_sentiment_classes=3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.sentence_classifier = torch.nn.Linear(768, num_sentence_classes)\n",
    "        self.sentiment_analysis = torch.nn.Linear(768, num_sentiment_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sentence_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.sentence_classifier(sentence_embeddings), self.sentiment_analysis(outputs.last_hidden_state)\n",
    "\n",
    "    \n",
    "model = MultiTaskLearningTransformer()\n",
    "class_logits, ner_logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "print(\"Class Logits:\", class_logits)\n",
    "print(\"NER Logits:\", ner_logits)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8920a889",
   "metadata": {},
   "source": [
    "1. If the entire network should be frozen.\n",
    "\n",
    "        The entire network should be frozen, means there is no weight updation. This is useful when we directly\n",
    "        want to use the pretrained network\n",
    "        \n",
    "2. If only the transformer backbone should be frozen.\n",
    "        Freezing the transformer backbone will freeze the weights of the transformer and only the task \n",
    "        specific heads are trained.This will be useful if we have a different task than which the transformer was\n",
    "        trained on\n",
    "        \n",
    "3. If only one of the task-specific heads (either for Task A or Task B) should be frozen.\n",
    "\n",
    "        Freezing one of the task-specific heads means that the weights for either Task A or Task B will not be updated during training. \n",
    "        This is useful when one task requires more adaptation than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889b800",
   "metadata": {},
   "source": [
    "# Task 3: Training Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8ae2222d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb89251ef6a4aaab6b41f0df40c362a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'task_a_label', 'task_b_labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 29\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = {\n",
    "    \"sentence\": [\n",
    "        \"Machine learning is fascinating.\",\n",
    "        \"Deep learning is a subset of machine learning.\",\n",
    "        \"Transformers are powerful models for NLP.\",\n",
    "        \"Social media is fake.\",\n",
    "        \"AI is revolutionizing healthcare.\",\n",
    "        \"Self-driving cars are the future.\",\n",
    "        \"AI is impacting almost every industry.\",\n",
    "        \"Machine learning algorithms are improving.\",\n",
    "        \"Artificial intelligence helps in decision making.\",\n",
    "        \"Social media addiction is harmful.\",\n",
    "        \"The internet is full of misinformation.\",\n",
    "        \"The future of technology is AI.\",\n",
    "        \"Natural language processing is transforming communication.\",\n",
    "        \"AI-driven tools are changing the way we work.\",\n",
    "        \"Automation through AI can help increase productivity.\",\n",
    "        \"Machine learning can help in analyzing big data.\",\n",
    "        \"AI can predict stock market trends.\",\n",
    "        \"Neural networks can be used in voice recognition.\",\n",
    "        \"Robots powered by AI are becoming common.\",\n",
    "        \"The AI market is growing rapidly.\",\n",
    "        \"Deep learning is a key part of modern AI.\",\n",
    "        \"Machine learning can solve complex problems.\",\n",
    "        \"AI is a driving force for innovation.\",\n",
    "        \"Social media platforms are increasing their influence.\",\n",
    "        \"Machine learning models can be used for fraud detection.\",\n",
    "        \"AI is used in facial recognition technologies.\",\n",
    "        \"The development of AI is accelerating.\",\n",
    "        \"AI can help in personalized healthcare.\",\n",
    "        \"Social media platforms are sources of fake news.\"\n",
    "    ],\n",
    "    \"task_a_label\": [1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n",
    "    \"task_b_labels\": [1, 0, 2, 2, 0, 0, 1, 0, 2, 0, 1, 2, 1, 0, 2, 1, 0, 1, 2, 1, 0, 0, 1, 0, 2, 2, 0, 2, 1]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "#Tokenization\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=False,  \n",
    "    )\n",
    "    \n",
    "    labels_task_b = []\n",
    "    for i, sentence in enumerate(examples[\"sentence\"]):\n",
    "        task_b_label = examples[\"task_b_labels\"][i]  \n",
    "\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  \n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(task_b_label)  \n",
    "            else:\n",
    "                label_ids.append(-100)  \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels_task_b.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"task_a_label\"] = examples[\"task_a_label\"]\n",
    "    tokenized_inputs[\"task_b_labels\"] = labels_task_b\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "358ada31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bfc1055af44865a6ce159ece0d8718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a183325f6304a9cbf3dd860ef388258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Classification Loss: 0.7931, NER Loss: 1.1368\n",
      "Classification Accuracy: 0.7391\n",
      "NER F1-Score: 0.2233\n",
      "Test Classification Accuracy: 0.3333\n",
      "Test NER F1-Score: 0.1237\n",
      "Epoch 2/3\n",
      "Classification Loss: 0.5337, NER Loss: 1.0949\n",
      "Classification Accuracy: 0.7391\n",
      "NER F1-Score: 0.3725\n",
      "Test Classification Accuracy: 0.3333\n",
      "Test NER F1-Score: 0.0000\n",
      "Epoch 3/3\n",
      "Classification Loss: 0.4164, NER Loss: 0.9890\n",
      "Classification Accuracy: 0.7826\n",
      "NER F1-Score: 0.3487\n",
      "Test Classification Accuracy: 0.6667\n",
      "Test NER F1-Score: 0.0317\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "class DatasetWrapper(TorchDataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.columns = dataset.column_names  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for key in self.columns:\n",
    "            if isinstance(self.dataset[key][idx], str):  \n",
    "                item[key] = self.dataset[key][idx]  \n",
    "            else:\n",
    "                item[key] = torch.tensor(self.dataset[key][idx])  \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "train_dataloader = DataLoader(DatasetWrapper(train_dataset), batch_size=2, shuffle=True)\n",
    "test_dataloader = DataLoader(DatasetWrapper(test_dataset), batch_size=2, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "model = MultiTaskTransformer().to(device)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "classification_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "ner_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)  \n",
    "\n",
    "num_epochs = 3  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_classification_loss = 0\n",
    "    total_ner_loss = 0\n",
    "    correct_classifications = 0\n",
    "    total_classifications = 0\n",
    "    all_ner_preds = []\n",
    "    all_ner_labels = []\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        task_a_labels = batch[\"task_a_label\"].to(device)\n",
    "        task_b_labels = batch[\"task_b_labels\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        class_logits, ner_logits = model(input_ids, attention_mask)\n",
    "        class_loss = classification_loss_fn(class_logits, task_a_labels)\n",
    "        batch_size, seq_len = ner_logits.size(0), ner_logits.size(1)\n",
    "        ner_logits_reshaped = ner_logits.view(batch_size * seq_len, -1)\n",
    "        task_b_labels_reshaped = task_b_labels.view(-1)\n",
    "        ner_loss = ner_loss_fn(ner_logits_reshaped, task_b_labels_reshaped)\n",
    "        total_loss = class_loss + ner_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_classification_loss += class_loss.item()\n",
    "        total_ner_loss += ner_loss.item()\n",
    "        class_preds = torch.argmax(class_logits, dim=-1)\n",
    "        correct_classifications += (class_preds == task_a_labels).sum().item()\n",
    "        total_classifications += task_a_labels.size(0)\n",
    "        ner_preds = torch.argmax(ner_logits, dim=-1)\n",
    "        ner_preds_flat = ner_preds.view(-1)\n",
    "        task_b_labels_reshaped_flat = task_b_labels_reshaped.view(-1)\n",
    "        valid_indices = task_b_labels_reshaped_flat != -100\n",
    "        ner_preds_valid = ner_preds_flat[valid_indices].cpu().numpy()\n",
    "        task_b_labels_valid = task_b_labels_reshaped_flat[valid_indices].cpu().numpy()\n",
    "        all_ner_preds.extend(ner_preds_valid)\n",
    "        all_ner_labels.extend(task_b_labels_valid)\n",
    "        \n",
    "    epoch_classification_loss = total_classification_loss / len(train_dataloader)\n",
    "    epoch_ner_loss = total_ner_loss / len(train_dataloader)\n",
    "    classification_accuracy = correct_classifications / total_classifications\n",
    "    f1 = f1_score(all_ner_labels, all_ner_preds, average='macro')\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Classification Loss: {epoch_classification_loss:.4f}, NER Loss: {epoch_ner_loss:.4f}\")\n",
    "    print(f\"Classification Accuracy: {classification_accuracy:.4f}\")\n",
    "    print(f\"NER F1-Score: {f1:.4f}\")\n",
    "    model.eval()\n",
    "    total_test_classification_loss = 0\n",
    "    total_test_ner_loss = 0\n",
    "    correct_test_classifications = 0\n",
    "    total_test_classifications = 0\n",
    "    all_test_ner_preds = []\n",
    "    all_test_ner_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            task_a_labels = batch[\"task_a_label\"].to(device)\n",
    "            task_b_labels = batch[\"task_b_labels\"].to(device)\n",
    "            class_logits, ner_logits = model(input_ids, attention_mask)\n",
    "            class_loss = classification_loss_fn(class_logits, task_a_labels)\n",
    "            batch_size, seq_len = ner_logits.size(0), ner_logits.size(1)\n",
    "            ner_logits_reshaped = ner_logits.view(batch_size * seq_len, -1)\n",
    "            task_b_labels_reshaped = task_b_labels.view(-1)\n",
    "            ner_loss = ner_loss_fn(ner_logits_reshaped, task_b_labels_reshaped)\n",
    "            total_test_classification_loss += class_loss.item()\n",
    "            total_test_ner_loss += ner_loss.item()\n",
    "            class_preds = torch.argmax(class_logits, dim=-1)\n",
    "            correct_test_classifications += (class_preds == task_a_labels).sum().item()\n",
    "            total_test_classifications += task_a_labels.size(0)\n",
    "            ner_preds = torch.argmax(ner_logits, dim=-1)\n",
    "            ner_preds_flat = ner_preds.view(-1)\n",
    "            task_b_labels_reshaped_flat = task_b_labels_reshaped.view(-1)\n",
    "            valid_indices = task_b_labels_reshaped_flat != -100\n",
    "            ner_preds_valid = ner_preds_flat[valid_indices].cpu().numpy()\n",
    "            task_b_labels_valid = task_b_labels_reshaped_flat[valid_indices].cpu().numpy()\n",
    "            all_test_ner_preds.extend(ner_preds_valid)\n",
    "            all_test_ner_labels.extend(task_b_labels_valid)\n",
    "        test_classification_accuracy = correct_test_classifications / total_test_classifications\n",
    "        test_f1 = f1_score(all_test_ner_labels, all_test_ner_preds, average='macro')\n",
    "        print(f\"Test Classification Accuracy: {test_classification_accuracy:.4f}\")\n",
    "        print(f\"Test NER F1-Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc48cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593eb9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada3a6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ca343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f6662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed79af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8026721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9cb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f63080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
