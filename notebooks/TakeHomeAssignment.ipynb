{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2409882",
   "metadata": {},
   "source": [
    "# Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de34edda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embeddings: tensor([[ 0.0559,  0.0093,  0.0912,  ..., -0.0607,  0.2878, -0.1385],\n",
      "        [ 0.0922, -0.2819,  0.3255,  ..., -0.1049,  0.2217, -0.3235]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "class SentenceTransformer(torch.nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.pooling = torch.nn.AdaptiveAvgPool1d(1) \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  \n",
    "        embeddings = self.pooling(last_hidden_state.permute(0, 2, 1)).squeeze(2)  \n",
    "        return embeddings\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = SentenceTransformer()\n",
    "\n",
    "sentences = [\"AI is transforming every Industry\", \"AI Agents are going to be the next big thing\"]\n",
    "inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "embeddings = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "print(\"Sentence Embeddings:\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1baea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embeddings: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence Embeddings:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366fa278",
   "metadata": {},
   "source": [
    "# Model Choice: I chose BERT as it does a pretty good job in various NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ed1bb7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Logits: tensor([[ 0.0443, -0.2693, -0.4749],\n",
      "        [ 0.0503, -0.2110, -0.5124]], grad_fn=<AddmmBackward0>)\n",
      "NER Logits: tensor([[[ 0.3535,  0.2386, -0.0754],\n",
      "         [ 0.3207,  0.2628, -0.0511],\n",
      "         [ 0.3275,  0.2744, -0.0660],\n",
      "         [ 0.3386,  0.2828, -0.0862]],\n",
      "\n",
      "        [[ 0.3175,  0.2936, -0.0594],\n",
      "         [ 0.2507,  0.3314, -0.0329],\n",
      "         [ 0.2584,  0.3437, -0.0470],\n",
      "         [ 0.2838,  0.3592, -0.0517]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskLearningTransformer(torch.nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', num_sentence_classes=3, num_sentiment_classes=3):\n",
    "        super(MultiTaskLearningTransformer, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Sentence classification head (3 classes)\n",
    "        self.sentence_classifier = torch.nn.Linear(768, num_sentence_classes)\n",
    "        \n",
    "        # Sentiment classification head (3 classes)\n",
    "        self.ner_classifier = torch.nn.Linear(768, num_sentiment_classes)  \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  \n",
    "\n",
    "        # Sentence classification logits\n",
    "        sentence_embeddings = last_hidden_state[:, 0, :]  \n",
    "        sentence_class_logits = self.sentence_classifier(sentence_embeddings) \n",
    "\n",
    "        # NER classification logits\n",
    "        ner_logits = self.ner_classifier(last_hidden_state)  \n",
    "\n",
    "        return sentence_class_logits, ner_logits\n",
    "    \n",
    "    \n",
    "model = MultiTaskLearningTransformer()\n",
    "class_logits, ner_logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "print(\"Class Logits:\", class_logits)\n",
    "print(\"NER Logits:\", ner_logits)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8920a889",
   "metadata": {},
   "source": [
    "1. If the entire network should be frozen.\n",
    "\n",
    "        The entire network should be frozen, means there is no weight updation. This is useful when we directly\n",
    "        want to use the pretrained network\n",
    "        \n",
    "2. If only the transformer backbone should be frozen.\n",
    "        Freezing the transformer backbone will freeze the weights of the transformer and only the task \n",
    "        specific heads are trained.This will be useful if we have a different task than which the transformer was\n",
    "        trained on\n",
    "        \n",
    "3. If only one of the task-specific heads (either for Task A or Task B) should be frozen.\n",
    "\n",
    "        Freezing one of the task-specific heads means that the weights for either Task A or Task B will not be updated during training. \n",
    "        This is useful when one task requires more adaptation than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889b800",
   "metadata": {},
   "source": [
    "# Task 3: Training Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8ae2222d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb89251ef6a4aaab6b41f0df40c362a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'task_a_label', 'task_b_labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 29\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = {\n",
    "    \"sentence\": [\n",
    "        \"Machine learning is fascinating.\",\n",
    "        \"Deep learning is a subset of machine learning.\",\n",
    "        \"Transformers are powerful models for NLP.\",\n",
    "        \"Social media is fake.\",\n",
    "        \"AI is revolutionizing healthcare.\",\n",
    "        \"Self-driving cars are the future.\",\n",
    "        \"AI is impacting almost every industry.\",\n",
    "        \"Machine learning algorithms are improving.\",\n",
    "        \"Artificial intelligence helps in decision making.\",\n",
    "        \"Social media addiction is harmful.\",\n",
    "        \"The internet is full of misinformation.\",\n",
    "        \"The future of technology is AI.\",\n",
    "        \"Natural language processing is transforming communication.\",\n",
    "        \"AI-driven tools are changing the way we work.\",\n",
    "        \"Automation through AI can help increase productivity.\",\n",
    "        \"Machine learning can help in analyzing big data.\",\n",
    "        \"AI can predict stock market trends.\",\n",
    "        \"Neural networks can be used in voice recognition.\",\n",
    "        \"Robots powered by AI are becoming common.\",\n",
    "        \"The AI market is growing rapidly.\",\n",
    "        \"Deep learning is a key part of modern AI.\",\n",
    "        \"Machine learning can solve complex problems.\",\n",
    "        \"AI is a driving force for innovation.\",\n",
    "        \"Social media platforms are increasing their influence.\",\n",
    "        \"Machine learning models can be used for fraud detection.\",\n",
    "        \"AI is used in facial recognition technologies.\",\n",
    "        \"The development of AI is accelerating.\",\n",
    "        \"AI can help in personalized healthcare.\",\n",
    "        \"Social media platforms are sources of fake news.\"\n",
    "    ],\n",
    "    \"task_a_label\": [1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n",
    "    \"task_b_labels\": [1, 0, 2, 2, 0, 0, 1, 0, 2, 0, 1, 2, 1, 0, 2, 1, 0, 1, 2, 1, 0, 0, 1, 0, 2, 2, 0, 2, 1]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "#Tokenization\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=False,  \n",
    "    )\n",
    "    \n",
    "    labels_task_b = []\n",
    "    for i, sentence in enumerate(examples[\"sentence\"]):\n",
    "        task_b_label = examples[\"task_b_labels\"][i]  \n",
    "\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  \n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(task_b_label)  \n",
    "            else:\n",
    "                label_ids.append(-100)  \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels_task_b.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"task_a_label\"] = examples[\"task_a_label\"]\n",
    "    tokenized_inputs[\"task_b_labels\"] = labels_task_b\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "358ada31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bfc1055af44865a6ce159ece0d8718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a183325f6304a9cbf3dd860ef388258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Classification Loss: 0.7931, NER Loss: 1.1368\n",
      "Classification Accuracy: 0.7391\n",
      "NER F1-Score: 0.2233\n",
      "Test Classification Accuracy: 0.3333\n",
      "Test NER F1-Score: 0.1237\n",
      "Epoch 2/3\n",
      "Classification Loss: 0.5337, NER Loss: 1.0949\n",
      "Classification Accuracy: 0.7391\n",
      "NER F1-Score: 0.3725\n",
      "Test Classification Accuracy: 0.3333\n",
      "Test NER F1-Score: 0.0000\n",
      "Epoch 3/3\n",
      "Classification Loss: 0.4164, NER Loss: 0.9890\n",
      "Classification Accuracy: 0.7826\n",
      "NER F1-Score: 0.3487\n",
      "Test Classification Accuracy: 0.6667\n",
      "Test NER F1-Score: 0.0317\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "class DatasetWrapper(TorchDataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.columns = dataset.column_names  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for key in self.columns:\n",
    "            if isinstance(self.dataset[key][idx], str):  \n",
    "                item[key] = self.dataset[key][idx]  \n",
    "            else:\n",
    "                item[key] = torch.tensor(self.dataset[key][idx])  \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "train_dataloader = DataLoader(DatasetWrapper(train_dataset), batch_size=2, shuffle=True)\n",
    "test_dataloader = DataLoader(DatasetWrapper(test_dataset), batch_size=2, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "model = MultiTaskTransformer().to(device)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "classification_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "ner_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)  \n",
    "\n",
    "num_epochs = 3  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_classification_loss = 0\n",
    "    total_ner_loss = 0\n",
    "    correct_classifications = 0\n",
    "    total_classifications = 0\n",
    "    all_ner_preds = []\n",
    "    all_ner_labels = []\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        task_a_labels = batch[\"task_a_label\"].to(device)\n",
    "        task_b_labels = batch[\"task_b_labels\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        class_logits, ner_logits = model(input_ids, attention_mask)\n",
    "        class_loss = classification_loss_fn(class_logits, task_a_labels)\n",
    "        batch_size, seq_len = ner_logits.size(0), ner_logits.size(1)\n",
    "        ner_logits_reshaped = ner_logits.view(batch_size * seq_len, -1)\n",
    "        task_b_labels_reshaped = task_b_labels.view(-1)\n",
    "        ner_loss = ner_loss_fn(ner_logits_reshaped, task_b_labels_reshaped)\n",
    "        total_loss = class_loss + ner_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_classification_loss += class_loss.item()\n",
    "        total_ner_loss += ner_loss.item()\n",
    "        class_preds = torch.argmax(class_logits, dim=-1)\n",
    "        correct_classifications += (class_preds == task_a_labels).sum().item()\n",
    "        total_classifications += task_a_labels.size(0)\n",
    "        ner_preds = torch.argmax(ner_logits, dim=-1)\n",
    "        ner_preds_flat = ner_preds.view(-1)\n",
    "        task_b_labels_reshaped_flat = task_b_labels_reshaped.view(-1)\n",
    "        valid_indices = task_b_labels_reshaped_flat != -100\n",
    "        ner_preds_valid = ner_preds_flat[valid_indices].cpu().numpy()\n",
    "        task_b_labels_valid = task_b_labels_reshaped_flat[valid_indices].cpu().numpy()\n",
    "        all_ner_preds.extend(ner_preds_valid)\n",
    "        all_ner_labels.extend(task_b_labels_valid)\n",
    "        \n",
    "    epoch_classification_loss = total_classification_loss / len(train_dataloader)\n",
    "    epoch_ner_loss = total_ner_loss / len(train_dataloader)\n",
    "    classification_accuracy = correct_classifications / total_classifications\n",
    "    f1 = f1_score(all_ner_labels, all_ner_preds, average='macro')\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Classification Loss: {epoch_classification_loss:.4f}, NER Loss: {epoch_ner_loss:.4f}\")\n",
    "    print(f\"Classification Accuracy: {classification_accuracy:.4f}\")\n",
    "    print(f\"NER F1-Score: {f1:.4f}\")\n",
    "    model.eval()\n",
    "    total_test_classification_loss = 0\n",
    "    total_test_ner_loss = 0\n",
    "    correct_test_classifications = 0\n",
    "    total_test_classifications = 0\n",
    "    all_test_ner_preds = []\n",
    "    all_test_ner_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            task_a_labels = batch[\"task_a_label\"].to(device)\n",
    "            task_b_labels = batch[\"task_b_labels\"].to(device)\n",
    "            class_logits, ner_logits = model(input_ids, attention_mask)\n",
    "            class_loss = classification_loss_fn(class_logits, task_a_labels)\n",
    "            batch_size, seq_len = ner_logits.size(0), ner_logits.size(1)\n",
    "            ner_logits_reshaped = ner_logits.view(batch_size * seq_len, -1)\n",
    "            task_b_labels_reshaped = task_b_labels.view(-1)\n",
    "            ner_loss = ner_loss_fn(ner_logits_reshaped, task_b_labels_reshaped)\n",
    "            total_test_classification_loss += class_loss.item()\n",
    "            total_test_ner_loss += ner_loss.item()\n",
    "            class_preds = torch.argmax(class_logits, dim=-1)\n",
    "            correct_test_classifications += (class_preds == task_a_labels).sum().item()\n",
    "            total_test_classifications += task_a_labels.size(0)\n",
    "            ner_preds = torch.argmax(ner_logits, dim=-1)\n",
    "            ner_preds_flat = ner_preds.view(-1)\n",
    "            task_b_labels_reshaped_flat = task_b_labels_reshaped.view(-1)\n",
    "            valid_indices = task_b_labels_reshaped_flat != -100\n",
    "            ner_preds_valid = ner_preds_flat[valid_indices].cpu().numpy()\n",
    "            task_b_labels_valid = task_b_labels_reshaped_flat[valid_indices].cpu().numpy()\n",
    "            all_test_ner_preds.extend(ner_preds_valid)\n",
    "            all_test_ner_labels.extend(task_b_labels_valid)\n",
    "        test_classification_accuracy = correct_test_classifications / total_test_classifications\n",
    "        test_f1 = f1_score(all_test_ner_labels, all_test_ner_preds, average='macro')\n",
    "        print(f\"Test Classification Accuracy: {test_classification_accuracy:.4f}\")\n",
    "        print(f\"Test NER F1-Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc48cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593eb9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada3a6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ca343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f6662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed79af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8026721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9cb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f63080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa756c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aaaaee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5809557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd5710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8604d04",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconll_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "conll_dataset[\"train\"].column_names[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b48cd6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "365debfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d60e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an official Python runtime as a parent image\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "COPY . /app\n",
    "\n",
    "# Install dependencies from requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Install specific version of PyTorch (2.0.1)\n",
    "RUN pip install torch==2.0.1\n",
    "\n",
    "# Run the application\n",
    "CMD [\"python\", \"main.py\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p310)",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
